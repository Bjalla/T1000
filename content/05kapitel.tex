%!TEX root = ../dokumentation.tex

\chapter{Fazit}

Ziel dieser Arbeit war es, Datencrawler für die einzelnen Komponenten der Social-Media-Plattform IBM Connections zu konzeptionieren und implementieren. Dafür wurde mit Hilfe der Porgrammiersprache Node.js ein Programm eintwickelt, welches die Daten beschafft, sortiert und zur Weiterverarbeitung strukturiert. 

Wichtig war hierbei, mögliche Fehler in der Verarbeitung der Daten zu beachten und abzufangen, sowie dafür zu sorgen, dass der Code möglichst effizient arbeitet. \\
Wichtig war, zunächst herauszufinden, in welcher Form die Daten aus der Seedlist ankommen und wie sie strukturiert sind, nachdem sie geparsed wurden. Im Anschluss daran konnte der Ablauf des Programms geplant werden, um diesen dann umzusetzen. Danach mussten die Daten in eine passende Form gebracht werden.

Daraus ergibt sich, dass für den Expertise Locator nun DatenCrawler für mehrere IBM Connections-Komponenten zur Verfügung stehen, um die Expertise Engine mit Daten zur Findung von Experten zu bestücken.